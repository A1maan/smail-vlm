{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T20:51:50.115954Z",
     "iopub.status.busy": "2025-11-02T20:51:50.115430Z",
     "iopub.status.idle": "2025-11-02T20:53:14.353383Z",
     "shell.execute_reply": "2025-11-02T20:53:14.352636Z",
     "shell.execute_reply.started": "2025-11-02T20:51:50.115932Z"
    },
    "id": "jVZYE5GrU33H",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q torch transformers pillow bitsandbytes accelerate open_clip_torch matplotlib pandas seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'chest_xray_samples/normal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-753886595.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnormal_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.jpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"normal_{i+1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'chest_xray_samples/normal'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir = \"chest_xray_samples\"\n",
    "normal_dir = os.path.join(data_dir, \"normal\")\n",
    "pneumonia_dir = os.path.join(data_dir, \"pneumonia\")\n",
    "\n",
    "images = {}\n",
    "\n",
    "normal_files = sorted([f for f in os.listdir(normal_dir) if f.endswith(('.jpeg', '.jpg', '.png'))])[:5]\n",
    "for i, f in enumerate(normal_files):\n",
    "    name = f\"normal_{i+1}\"\n",
    "    images[name] = {\"path\": os.path.join(normal_dir, f), \"image\": Image.open(os.path.join(normal_dir, f)).convert(\"RGB\"), \"condition\": \"normal\"}\n",
    "\n",
    "pneumonia_files = sorted([f for f in os.listdir(pneumonia_dir) if f.endswith(('.jpeg', '.jpg', '.png'))])[:5]\n",
    "for i, f in enumerate(pneumonia_files):\n",
    "    name = f\"pneumonia_{i+1}\"\n",
    "    images[name] = {\"path\": os.path.join(pneumonia_dir, f), \"image\": Image.open(os.path.join(pneumonia_dir, f)).convert(\"RGB\"), \"condition\": \"pneumonia\"}\n",
    "\n",
    "print(f\"loaded {len(images)} images\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for name, data in images.items():\n",
    "    row = 0 if \"normal\" in name else 1\n",
    "    col = int(name.split(\"_\")[1]) - 1\n",
    "    axes[row, col].imshow(data[\"image\"], cmap='gray')\n",
    "    axes[row, col].set_title(name)\n",
    "    axes[row, col].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts(condition):\n",
    "    if condition == \"normal\":\n",
    "        return [\n",
    "            (\"Is there pneumonia in this X-ray?\", False, \"affirmative\"),\n",
    "            (\"Is there a pleural effusion in this X-ray?\", False, \"affirmative\"),\n",
    "            (\"Is there no pneumonia in this X-ray?\", True, \"negated\"),\n",
    "            (\"Is there no pleural effusion in this X-ray?\", True, \"negated\"),\n",
    "            (\"Is there right-sided pneumonia?\", False, \"left_right\"),\n",
    "            (\"Is there left-sided pneumonia?\", False, \"left_right\"),\n",
    "        ]\n",
    "    else:\n",
    "        return [\n",
    "            (\"Is there pneumonia in this X-ray?\", True, \"affirmative\"),\n",
    "            (\"Is there consolidation in this X-ray?\", True, \"affirmative\"),\n",
    "            (\"Is there no pneumonia in this X-ray?\", False, \"negated\"),\n",
    "            (\"Is there no lung abnormality in this X-ray?\", False, \"negated\"),\n",
    "            (\"Is there right-sided pneumonia?\", True, \"left_right\"),\n",
    "            (\"Is there left-sided pneumonia?\", True, \"left_right\"),\n",
    "        ]\n",
    "\n",
    "all_test_cases = []\n",
    "for img_name, data in images.items():\n",
    "    for prompt, expected, prompt_type in get_prompts(data[\"condition\"]):\n",
    "        all_test_cases.append({\n",
    "            \"image_name\": img_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"ground_truth\": expected,\n",
    "            \"prompt_type\": prompt_type,\n",
    "            \"condition\": data[\"condition\"],\n",
    "        })\n",
    "\n",
    "print(f\"total: {len(all_test_cases)} test cases\")\n",
    "print(f\"affirmative: {sum(1 for x in all_test_cases if x['prompt_type'] == 'affirmative')}\")\n",
    "print(f\"negated: {sum(1 for x in all_test_cases if x['prompt_type'] == 'negated')}\")\n",
    "print(f\"left_right: {sum(1 for x in all_test_cases if x['prompt_type'] == 'left_right')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading LLaVA-Med**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "\n",
    "model_path = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "print(f\"loaded {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, processor, image, prompt, max_new_tokens=128):\n",
    "    conversation = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": prompt}]}]\n",
    "    text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = processor(images=image, text=text_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, output_scores=True, return_dict_in_generate=True)\n",
    "    \n",
    "    generated_ids = outputs.sequences[:, inputs.input_ids.shape[1]:]\n",
    "    response = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    \n",
    "    transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, normalize_logits=True)\n",
    "    mean_logprob = transition_scores[0].mean().item()\n",
    "    \n",
    "    response_lower = response.lower()\n",
    "    yes_indicators = [\"yes\", \"there is\", \"present\", \"visible\", \"shows\", \"appears\"]\n",
    "    no_indicators = [\"no\", \"there is no\", \"not present\", \"not visible\", \"no evidence\"]\n",
    "    \n",
    "    says_yes = any(ind in response_lower for ind in yes_indicators)\n",
    "    says_no = any(ind in response_lower for ind in no_indicators)\n",
    "    \n",
    "    if says_yes and says_no:\n",
    "        says_yes = response_lower.find(\"yes\") < response_lower.find(\"no\") if \"yes\" in response_lower and \"no\" in response_lower else says_yes\n",
    "    \n",
    "    return response, mean_logprob, says_yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "\n",
    "for tc in tqdm(all_test_cases):\n",
    "    img = images[tc[\"image_name\"]][\"image\"]\n",
    "    response, logprob, says_yes = run_inference(model, processor, img, tc[\"prompt\"])\n",
    "    \n",
    "    is_correct = says_yes == tc[\"ground_truth\"]\n",
    "    \n",
    "    results.append({\n",
    "        \"image\": tc[\"image_name\"],\n",
    "        \"prompt\": tc[\"prompt\"],\n",
    "        \"prompt_type\": tc[\"prompt_type\"],\n",
    "        \"ground_truth\": tc[\"ground_truth\"],\n",
    "        \"response\": response[:150],\n",
    "        \"model_says_yes\": says_yes,\n",
    "        \"is_correct\": is_correct,\n",
    "        \"mean_logprob\": logprob,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(f\"completed {len(df)} test cases\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Results Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HALLUCINATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\noverall accuracy: {df['is_correct'].mean() * 100:.1f}%\")\n",
    "\n",
    "print(\"\\nby prompt type:\")\n",
    "for ptype in [\"affirmative\", \"negated\", \"left_right\"]:\n",
    "    subset = df[df[\"prompt_type\"] == ptype]\n",
    "    acc = subset[\"is_correct\"].mean() * 100\n",
    "    print(f\"  {ptype}: {acc:.1f}%\")\n",
    "\n",
    "negated = df[df[\"prompt_type\"] == \"negated\"]\n",
    "false_negated = negated[negated[\"ground_truth\"] == False]\n",
    "halluc_rate = false_negated[\"model_says_yes\"].mean() * 100 if len(false_negated) > 0 else 0\n",
    "print(f\"\\nhallucination rate (yes to negated): {halluc_rate:.1f}%\")\n",
    "\n",
    "print(\"\\nlog-probability:\")\n",
    "correct = df[df[\"is_correct\"] == True][\"mean_logprob\"]\n",
    "incorrect = df[df[\"is_correct\"] == False][\"mean_logprob\"]\n",
    "print(f\"  correct: {correct.mean():.4f}\")\n",
    "print(f\"  incorrect: {incorrect.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# accuracy by prompt type\n",
    "acc_by_type = df.groupby(\"prompt_type\")[\"is_correct\"].mean() * 100\n",
    "acc_by_type.plot(kind=\"bar\", ax=axes[0], color=[\"#2ecc71\", \"#e74c3c\", \"#3498db\"])\n",
    "axes[0].set_title(\"accuracy by prompt type\")\n",
    "axes[0].set_ylabel(\"accuracy (%)\")\n",
    "axes[0].set_ylim(0, 100)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# log prob distribution\n",
    "df[\"correctness\"] = df[\"is_correct\"].map({True: \"correct\", False: \"incorrect\"})\n",
    "sns.boxplot(data=df, x=\"correctness\", y=\"mean_logprob\", ax=axes[1], palette={\"correct\": \"#2ecc71\", \"incorrect\": \"#e74c3c\"})\n",
    "axes[1].set_title(\"log-prob by correctness\")\n",
    "\n",
    "# negated prompt responses\n",
    "negated_df = df[df[\"prompt_type\"] == \"negated\"]\n",
    "by_truth = negated_df.groupby(\"ground_truth\")[\"model_says_yes\"].mean() * 100\n",
    "axes[2].bar([\"expected: no\", \"expected: yes\"], [by_truth.get(False, 0), by_truth.get(True, 0)], color=[\"#e74c3c\", \"#2ecc71\"])\n",
    "axes[2].set_title(\"'yes' rate on negated prompts\")\n",
    "axes[2].set_ylabel(\"% saying yes\")\n",
    "axes[2].set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BiomedCLIP Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch.nn.functional as F\n",
    "\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "tokenizer = open_clip.get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = clip_model.to(device).eval()\n",
    "print(f\"loaded biomedclip on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_similarity(image, text):\n",
    "    img_input = preprocess(image).unsqueeze(0).to(device)\n",
    "    txt_input = tokenizer([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        img_feat = F.normalize(clip_model.encode_image(img_input), dim=-1)\n",
    "        txt_feat = F.normalize(clip_model.encode_text(txt_input), dim=-1)\n",
    "        return (img_feat @ txt_feat.T).item()\n",
    "\n",
    "clip_results = []\n",
    "for tc in tqdm(all_test_cases):\n",
    "    img = images[tc[\"image_name\"]][\"image\"]\n",
    "    sim = clip_similarity(img, tc[\"prompt\"])\n",
    "    clip_results.append({\"image\": tc[\"image_name\"], \"prompt\": tc[\"prompt\"], \"prompt_type\": tc[\"prompt_type\"], \"ground_truth\": tc[\"ground_truth\"], \"clip_similarity\": sim})\n",
    "\n",
    "df_clip = pd.DataFrame(clip_results)\n",
    "df = df.merge(df_clip[[\"image\", \"prompt\", \"clip_similarity\"]], on=[\"image\", \"prompt\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nbiomedclip similarity:\")\n",
    "print(f\"  true statements: {df[df['ground_truth'] == True]['clip_similarity'].mean():.4f}\")\n",
    "print(f\"  false statements: {df[df['ground_truth'] == False]['clip_similarity'].mean():.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "df[\"truth_label\"] = df[\"ground_truth\"].map({True: \"true\", False: \"false\"})\n",
    "sns.boxplot(data=df, x=\"truth_label\", y=\"clip_similarity\", ax=ax, palette={\"true\": \"#2ecc71\", \"false\": \"#e74c3c\"})\n",
    "ax.set_title(\"biomedclip similarity by ground truth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"HALLUCINATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "halluc_data = []\n",
    "for ptype in [\"affirmative\", \"negated\", \"left_right\"]:\n",
    "    subset = df[df[\"prompt_type\"] == ptype]\n",
    "    false_stmt = subset[subset[\"ground_truth\"] == False]\n",
    "    halluc = false_stmt[\"model_says_yes\"].mean() * 100 if len(false_stmt) > 0 else 0\n",
    "    halluc_data.append({\"prompt_type\": ptype, \"total\": len(subset), \"accuracy\": f\"{subset['is_correct'].mean()*100:.1f}%\", \"halluc_rate\": f\"{halluc:.1f}%\"})\n",
    "print(pd.DataFrame(halluc_data).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LOG-PROBABILITY DIFFERENCES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "logprob_data = []\n",
    "for ptype in [\"affirmative\", \"negated\", \"left_right\"]:\n",
    "    subset = df[df[\"prompt_type\"] == ptype]\n",
    "    corr = subset[subset[\"is_correct\"] == True][\"mean_logprob\"]\n",
    "    incorr = subset[subset[\"is_correct\"] == False][\"mean_logprob\"]\n",
    "    logprob_data.append({\n",
    "        \"prompt_type\": ptype,\n",
    "        \"logprob_correct\": f\"{corr.mean():.4f}\" if len(corr) > 0 else \"n/a\",\n",
    "        \"logprob_incorrect\": f\"{incorr.mean():.4f}\" if len(incorr) > 0 else \"n/a\",\n",
    "    })\n",
    "print(pd.DataFrame(logprob_data).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BIOMEDCLIP SIMILARITY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "clip_data = []\n",
    "for ptype in [\"affirmative\", \"negated\", \"left_right\"]:\n",
    "    subset = df[df[\"prompt_type\"] == ptype]\n",
    "    true_s = subset[subset[\"ground_truth\"] == True][\"clip_similarity\"]\n",
    "    false_s = subset[subset[\"ground_truth\"] == False][\"clip_similarity\"]\n",
    "    clip_data.append({\n",
    "        \"prompt_type\": ptype,\n",
    "        \"sim_true\": f\"{true_s.mean():.4f}\" if len(true_s) > 0 else \"n/a\",\n",
    "        \"sim_false\": f\"{false_s.mean():.4f}\" if len(false_s) > 0 else \"n/a\",\n",
    "    })\n",
    "print(pd.DataFrame(clip_data).to_string(index=False))\n",
    "\n",
    "df.to_csv(\"hallucination_results.csv\", index=False)\n",
    "print(\"\\nsaved to hallucination_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **9. MiniGPT-Med**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q psutil==5.9.4 regex==2022.10.31 tqdm==4.64.1 timm==0.6.13 webdataset==0.2.48 omegaconf==2.3.0 opencv-python==4.7.0.72 decord==0.6.0 peft==0.2.0 sentence-transformers gradio==3.47.1 accelerate==0.20.3 scikit-image visual-genome wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"MiniGPT-Med\"):\n",
    "    !git clone https://github.com/Vision-CAIR/MiniGPT-Med.git\n",
    "%cd MiniGPT-Med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.registry import registry\n",
    "from minigpt4.conversation.conversation import Chat, CONV_VISION_minigptv2\n",
    "\n",
    "import argparse\n",
    "\n",
    "# config paths - adjust these for your setup\n",
    "cfg_path = \"eval_configs/minigptv2_eval.yaml\"\n",
    "model_ckpt = \"/kaggle/input/minigpt-med/pytorch/default/1/miniGPT_Med.pth\"  # adjust path\n",
    "llama_path = \"Llama-2-7b-chat-hf\"  # adjust path\n",
    "\n",
    "# update config\n",
    "import yaml\n",
    "with open(cfg_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "config['model']['ckpt'] = model_ckpt\n",
    "with open(cfg_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "args = argparse.Namespace(cfg_path=cfg_path, gpu_id=0, options=[])\n",
    "cfg = Config(args)\n",
    "\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = 0\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "minigpt_model = model_cls.from_config(model_config).to('cuda:0')\n",
    "\n",
    "vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "\n",
    "chat = Chat(minigpt_model, vis_processor, device='cuda:0')\n",
    "print(\"loaded minigpt-med\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_minigpt_inference(chat, image, prompt):\n",
    "    chat_state = CONV_VISION_minigptv2.copy()\n",
    "    img_list = []\n",
    "    \n",
    "    chat.upload_img(image, chat_state, img_list)\n",
    "    chat.encode_img(img_list)\n",
    "    chat.ask(prompt, chat_state)\n",
    "    \n",
    "    response = chat.answer(\n",
    "        conv=chat_state,\n",
    "        img_list=img_list,\n",
    "        num_beams=1,\n",
    "        temperature=1.0,\n",
    "        max_new_tokens=128,\n",
    "        max_length=2000\n",
    "    )[0]\n",
    "    \n",
    "    response_lower = response.lower()\n",
    "    yes_indicators = [\"yes\", \"there is\", \"present\", \"visible\", \"shows\", \"appears\"]\n",
    "    no_indicators = [\"no\", \"there is no\", \"not present\", \"not visible\", \"no evidence\"]\n",
    "    \n",
    "    says_yes = any(ind in response_lower for ind in yes_indicators)\n",
    "    says_no = any(ind in response_lower for ind in no_indicators)\n",
    "    \n",
    "    if says_yes and says_no:\n",
    "        says_yes = response_lower.find(\"yes\") < response_lower.find(\"no\") if \"yes\" in response_lower and \"no\" in response_lower else says_yes\n",
    "    \n",
    "    return response, says_yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload images (paths may have changed after cd)\n",
    "%cd ..\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "images_minigpt = {}\n",
    "for name in [\"normal_1\", \"normal_2\", \"normal_3\", \"normal_4\", \"normal_5\"]:\n",
    "    idx = int(name.split(\"_\")[1]) - 1\n",
    "    f = sorted([f for f in os.listdir(\"chest_xray_samples/normal\") if f.endswith(('.jpeg', '.jpg', '.png'))])[idx]\n",
    "    images_minigpt[name] = PILImage.open(f\"chest_xray_samples/normal/{f}\").convert(\"RGB\")\n",
    "\n",
    "for name in [\"pneumonia_1\", \"pneumonia_2\", \"pneumonia_3\", \"pneumonia_4\", \"pneumonia_5\"]:\n",
    "    idx = int(name.split(\"_\")[1]) - 1\n",
    "    f = sorted([f for f in os.listdir(\"chest_xray_samples/pneumonia\") if f.endswith(('.jpeg', '.jpg', '.png'))])[idx]\n",
    "    images_minigpt[name] = PILImage.open(f\"chest_xray_samples/pneumonia/{f}\").convert(\"RGB\")\n",
    "\n",
    "print(f\"loaded {len(images_minigpt)} images for minigpt-med\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minigpt_results = []\n",
    "\n",
    "for tc in tqdm(all_test_cases):\n",
    "    img = images_minigpt[tc[\"image_name\"]]\n",
    "    response, says_yes = run_minigpt_inference(chat, img, tc[\"prompt\"])\n",
    "    \n",
    "    is_correct = says_yes == tc[\"ground_truth\"]\n",
    "    \n",
    "    minigpt_results.append({\n",
    "        \"image\": tc[\"image_name\"],\n",
    "        \"prompt\": tc[\"prompt\"],\n",
    "        \"prompt_type\": tc[\"prompt_type\"],\n",
    "        \"ground_truth\": tc[\"ground_truth\"],\n",
    "        \"response\": response[:150],\n",
    "        \"model_says_yes\": says_yes,\n",
    "        \"is_correct\": is_correct,\n",
    "        \"model\": \"MiniGPT-Med\"\n",
    "    })\n",
    "\n",
    "df_minigpt = pd.DataFrame(minigpt_results)\n",
    "print(f\"completed {len(df_minigpt)} test cases for minigpt-med\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MINIGPT-MED RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\noverall accuracy: {df_minigpt['is_correct'].mean() * 100:.1f}%\")\n",
    "\n",
    "print(\"\\nby prompt type:\")\n",
    "for ptype in [\"affirmative\", \"negated\", \"left_right\"]:\n",
    "    subset = df_minigpt[df_minigpt[\"prompt_type\"] == ptype]\n",
    "    acc = subset[\"is_correct\"].mean() * 100\n",
    "    print(f\"  {ptype}: {acc:.1f}%\")\n",
    "\n",
    "negated = df_minigpt[df_minigpt[\"prompt_type\"] == \"negated\"]\n",
    "false_negated = negated[negated[\"ground_truth\"] == False]\n",
    "halluc_rate = false_negated[\"model_says_yes\"].mean() * 100 if len(false_negated) > 0 else 0\n",
    "print(f\"\\nhallucination rate (yes to negated): {halluc_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **10. Compare LLaVA vs MiniGPT-Med**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"model\"] = \"LLaVA\"\n",
    "df_combined = pd.concat([df[[\"image\", \"prompt\", \"prompt_type\", \"ground_truth\", \"response\", \"model_says_yes\", \"is_correct\", \"model\"]], df_minigpt], ignore_index=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison = []\n",
    "for model_name in [\"LLaVA\", \"MiniGPT-Med\"]:\n",
    "    subset = df_combined[df_combined[\"model\"] == model_name]\n",
    "    \n",
    "    acc = subset[\"is_correct\"].mean() * 100\n",
    "    \n",
    "    neg = subset[subset[\"prompt_type\"] == \"negated\"]\n",
    "    false_neg = neg[neg[\"ground_truth\"] == False]\n",
    "    halluc = false_neg[\"model_says_yes\"].mean() * 100 if len(false_neg) > 0 else 0\n",
    "    \n",
    "    aff_acc = subset[subset[\"prompt_type\"] == \"affirmative\"][\"is_correct\"].mean() * 100\n",
    "    neg_acc = subset[subset[\"prompt_type\"] == \"negated\"][\"is_correct\"].mean() * 100\n",
    "    lr_acc = subset[subset[\"prompt_type\"] == \"left_right\"][\"is_correct\"].mean() * 100\n",
    "    \n",
    "    comparison.append({\n",
    "        \"model\": model_name,\n",
    "        \"overall_acc\": f\"{acc:.1f}%\",\n",
    "        \"affirmative_acc\": f\"{aff_acc:.1f}%\",\n",
    "        \"negated_acc\": f\"{neg_acc:.1f}%\",\n",
    "        \"left_right_acc\": f\"{lr_acc:.1f}%\",\n",
    "        \"halluc_rate\": f\"{halluc:.1f}%\"\n",
    "    })\n",
    "\n",
    "print(pd.DataFrame(comparison).to_string(index=False))\n",
    "\n",
    "df_combined.to_csv(\"hallucination_results_both_models.csv\", index=False)\n",
    "print(\"\\nsaved to hallucination_results_both_models.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# accuracy comparison\n",
    "acc_data = df_combined.groupby([\"model\", \"prompt_type\"])[\"is_correct\"].mean().unstack() * 100\n",
    "acc_data.plot(kind=\"bar\", ax=axes[0], color=[\"#2ecc71\", \"#e74c3c\", \"#3498db\"])\n",
    "axes[0].set_title(\"accuracy by model and prompt type\")\n",
    "axes[0].set_ylabel(\"accuracy (%)\")\n",
    "axes[0].set_ylim(0, 100)\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "axes[0].legend(title=\"prompt type\")\n",
    "\n",
    "# hallucination comparison\n",
    "halluc_by_model = []\n",
    "for model_name in [\"LLaVA\", \"MiniGPT-Med\"]:\n",
    "    subset = df_combined[df_combined[\"model\"] == model_name]\n",
    "    neg = subset[(subset[\"prompt_type\"] == \"negated\") & (subset[\"ground_truth\"] == False)]\n",
    "    halluc = neg[\"model_says_yes\"].mean() * 100 if len(neg) > 0 else 0\n",
    "    halluc_by_model.append(halluc)\n",
    "\n",
    "axes[1].bar([\"LLaVA\", \"MiniGPT-Med\"], halluc_by_model, color=[\"#3498db\", \"#9b59b6\"])\n",
    "axes[1].set_title(\"hallucination rate (yes to false negated)\")\n",
    "axes[1].set_ylabel(\"% saying yes\")\n",
    "axes[1].set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_comparison.png\", dpi=150)\n",
    "plt.show()\n",
    "print(\"saved to model_comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8634370,
     "sourceId": 13589738,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 488494,
     "modelInstanceId": 472608,
     "sourceId": 627626,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
